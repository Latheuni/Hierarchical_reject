{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This is a guide on how to use the functions in this repository. Be aware, I assume here that this notebook is ran inside the folder Hierarchical_reject of the repository. If you download this notebook and run it outside of this folder, you will have to adapt the function paths when you import them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following analyses, we will assume that the AMB dataset is downloaded from [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10213715.svg)](https://doi.org/10.5281/zenodo.10213715)\n",
    "and that the paths to the data and label files are specified down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMBPath = ...\n",
    "LabelsAMBPath = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocessing of the data can be easily performed with the help of the dataset-specific preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.Preprocessing_AMB import Preprocessing_AMB\n",
    "Data, Labels = Preprocessing_AMB(AMBPath, LabelsAMBPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat annotation and evaluation\n",
    "The AMB Data is not loaded in under a sparse format during preprocessing (this will only be the case for the Azimuth PBMC dataset) and thus the non-sparse functions can be used. \n",
    "\n",
    "In the following block of code, we show how to perform flat annotation with 5-fold cross-validation and HVG selection, with the logistic regression classifier of scikit-klearn. The regularization strength parameter of the Logistic Regression classifier ('C') will also be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Run.General_analyses import Run_Flat_KF, SaveResultsKF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Define the classifier and parameters\n",
    "clf = LogisticRegression(penalty = 'l2', multi_class = 'multinomial', n_jobs = 1)\n",
    "params = {'C': [1,100,10000], 'top_genes' : [10 000, 30 000, 50 000]}\n",
    "\n",
    "## Run the analyses\n",
    "Predictions, Probs, Act, Acc, Bestparams, Classifiers, Xtest, ytests =Run_Flat_KF(clf, 5, Data, Labels, params, Norm = True, HVG = True, save_clf = True)\n",
    "\n",
    "## (Optional) save the results\n",
    "dir_ = ...\n",
    "name = ...\n",
    "\n",
    "SaveResultsKF(Predictions, Act, Acc, Bestparams, dir_, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the accuracy score or other metrics can be calculated. \n",
    "\n",
    "To construct Accuracy-rejection curves, the Evaluation_AR_Flat function can be used.\n",
    "\n",
    "Note that the AMB label hierarchy is balanced (all cell type labels have three levels), unlike all the other datasets in the repository [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10213715.svg)](https://doi.org/10.5281/zenodo.10213715)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Functions_Accuracy_Reject import Evaluate_AR_Flat\n",
    "\n",
    "results = Evaluate_AR_Flat(Classifiers, Xtest, ytests, Predictions, Probs, b = True, scores = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# accuracy rejection curves\n",
    "plt.plot(results['steps'], results['acc'])\n",
    "\n",
    "# rejection percentage curves\n",
    "plt.plot(results['steps'], results['perc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical annotation and evaluation\n",
    "\n",
    "The same set-up as above is illustrated here, only with hierarchical annotation instead of flat annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Run.General_analyses import Run_H_KF, SaveResultsKF\n",
    "from Evaluation.Functions_Accuracy_Reject import Evaluate_AR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Define the classifier and parameters\n",
    "clf = LogisticRegression(penalty = 'l2', multi_class = 'multinomial', n_jobs = 1)\n",
    "params = {'C': [1,100,10000], 'top_genes' : [10 000, 30 000, 50 000]}\n",
    "\n",
    "## Run the analyses\n",
    "Predictions, Probs, Act, Acc, Bestparams, Classifiers, Xtests, ytests =Run_H_KF(clf, 5, Data, Labels, params, 1, None, greedy_ = False, Norm = True, HVG = True, save_clf = True)\n",
    "# Note: for the number of cores, be careful as n_jobs (classifier) * n_jobsHCL can be used\n",
    "# If you don't want to make accuracy rejection curves, but just perform partial rejection directly, modify reject_thresh. \n",
    "# Full rejection can easily be applied through simple thresholding aftwards, based on the entire label\n",
    "\n",
    "## (Optional) save the results\n",
    "dir_ = ...\n",
    "name = ...\n",
    "\n",
    "SaveResultsKF(Predictions, Act, Acc, Bestparams, dir_, name)\n",
    "\n",
    "results = Evaluate_AR(Classifiers, Xtests, ytests, Predictions, greedy = False)\n",
    "\n",
    "# accuracy rejection curves\n",
    "plt.plot(results['steps'], results['acc'])\n",
    "\n",
    "# rejection percentage curves\n",
    "plt.plot(results['steps'], results['perc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
