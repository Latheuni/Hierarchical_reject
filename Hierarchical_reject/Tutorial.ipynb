{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This is a guide on how to use the functions in this repository. Be aware, I assume here that I am inside the folder Hierarchicl_reject of the repository. If you download this folder and use the functions outside of this folder, you will have to adapt the function paths when you import them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following analyses, we will assume that the AMB dataset is downloaded from [https://zenodo.org/records/3357167] and that the paths to the data files are specified down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMBPath = ...\n",
    "LabelsAMBPath = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocessing can be performed easily just by using the correct preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.Preprocessing_AMB import Preprocessing_AMB\n",
    "Data, Labels = Preprocessing_AMB(AMBPath, LabelsAMBPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat annotation and evaluation\n",
    "The AMB Data is not present in a sparse format (this is only the case for the Azimuth PBMC dataset) and thus the non-sparse functions can be used. Let's say we want to run flat annotation with 5-fold cross-validation and HVG selection. Together with the Logistic regression classifier of sklearn and we also want to tune the regularization strength parameter of the Logistic Regression classifier ('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Run.General_analyses import Run_Flat_KF, SaveResultsKF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Define the classifier and parameters\n",
    "clf = LogisticRegression(penalty = 'l2', multi_class = 'multinomial', n_jobs = 1)\n",
    "params = {'C': [1,100,10000], 'top_genes' : [10 000, 30 000, 50 000]}\n",
    "\n",
    "## Run the analyses\n",
    "Predictions, Probs, Act, Acc, Bestparams, Classifiers, Xtest, ytests =Run_Flat_KF(clf, 5, Data, Labels, params, Norm = True, HVG = True, save_clf = True)\n",
    "\n",
    "## (Optional) save the results\n",
    "dir_ = ...\n",
    "name = ...\n",
    "\n",
    "SaveResultsKF(Predictions, Act, Acc, Bestparams, dir_, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results the accuracy score or balanced accuracy score or other metrics can be calculated. To construct Accuracy-rejection curves, the Evaluation_AR_Flat function can be used. The AMB label hierarchy is balanced (unlike all the other datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Functions_Accuracy_Reject import Evaluate_AR_Flat\n",
    "\n",
    "results = Evaluate_AR_Flat(Classifiers, Xtest, ytests, Predictions, Probs, b = True, scores = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# accuracy rejection curves\n",
    "plt.plot(results['steps'], results['acc'])\n",
    "\n",
    "# rejection percentage curves\n",
    "plt.plot(results['steps'], results['perc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical annotation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Run.General_analyses import Run_H_KF, SaveResultsKF\n",
    "from Evaluation.Functions_Accuracy_Reject import Evaluate_AR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Define the classifier and parameters\n",
    "clf = LogisticRegression(penalty = 'l2', multi_class = 'multinomial', n_jobs = 1)\n",
    "params = {'C': [1,100,10000], 'top_genes' : [10 000, 30 000, 50 000]}\n",
    "\n",
    "## Run the analyses\n",
    "Predictions, Probs, Act, Acc, Bestparams, Classifiers, Xtests, ytests =Run_H_KF(clf, 5, Data, Labels, params, 1, None, greedy_ = False, Norm = True, HVG = True, save_clf = True)\n",
    "# Note: for the number of cores, be careful as n_jobs (classifier) * n_jobsHCL can be used\n",
    "# If you don't want to make accuracy rejection curves, but just perform partial rejection directly, modify reject_thresh. \n",
    "# Full rejection can easily be applied through simple thresholding aftwards, based on the entire label\n",
    "\n",
    "## (Optional) save the results\n",
    "dir_ = ...\n",
    "name = ...\n",
    "\n",
    "SaveResultsKF(Predictions, Act, Acc, Bestparams, dir_, name)\n",
    "\n",
    "results = Evaluate_AR(Classifiers, Xtests, ytests, Predictions, greedy = False)\n",
    "\n",
    "# accuracy rejection curves\n",
    "plt.plot(results['steps'], results['acc'])\n",
    "\n",
    "# rejection percentage curves\n",
    "plt.plot(results['steps'], results['perc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
